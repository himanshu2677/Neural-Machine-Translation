{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "sITNYYVwBIss"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mH7Hx3hvLi-",
        "outputId": "eb9b5184-0c77-4c2f-b8c8-10847d18a852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7vUwlEE0vJ5y"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N5bKv1Ld0AGE"
      },
      "outputs": [],
      "source": [
        "processed_data_location = \"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/processed_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessed data, vectorizers and embedding layers for english and hindi text"
      ],
      "metadata": {
        "id": "IqQLyPQGvxKn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GN4Oo_GuvShs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee56938-cfb1-49a9-c227-76144f412319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training examples: 1448877\n"
          ]
        }
      ],
      "source": [
        "with open(processed_data_location + \"/train.pkl\",'rb') as f:\n",
        "  train = pickle.load(f)\n",
        "\n",
        "print(\"Number of Training examples:\", len(train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "caitnkkk0Km-"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "random.shuffle(train)\n",
        "train_size = len(train)\n",
        "train, valid = train[:int(train_size*0.9)],train[int(train_size*0.9):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4JqQhaBDi4sY"
      },
      "outputs": [],
      "source": [
        "def load_vectorizer(location):\n",
        "  import pickle\n",
        "  from_disk = pickle.load(open(location, \"rb\"))\n",
        "  vectorizer = layers.TextVectorization.from_config(from_disk['config'])\n",
        "  vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "  vectorizer.set_weights(from_disk['weights'])\n",
        "  return vectorizer\n",
        "\n",
        "en_fasttext_vectorizer = load_vectorizer('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/EN_Fasttext_Vectorizer.pkl')\n",
        "hi_fasttext_vectorizer = load_vectorizer('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/HI_Fasttext_Vectorizer.pkl')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding(location):\n",
        "  import pickle\n",
        "  from_disk = pickle.load(open(location, \"rb\"))\n",
        "  embedding_layer = layers.Embedding.from_config(from_disk['config'])\n",
        "\n",
        "  return embedding_layer\n",
        "\n",
        "en_embedding_layer = load_embedding(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/en_fasttext_embedding_layer.pkl\")\n",
        "hi_embedding_layer = load_embedding(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/hi_fasttext_embedding_layer.pkl\")\n"
      ],
      "metadata": {
        "id": "QcqCiGsbwHde"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "ZF1u7PjQwLnk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UIO3awPCDkQs"
      },
      "outputs": [],
      "source": [
        "def create_tf_dataset(data):\n",
        "  data = [(text['en'],text['hi']) for text in data]\n",
        "  tf_data = tf.data.Dataset.from_tensor_slices(data)\n",
        "  tf_data = tf_data.shuffle(BATCH_SIZE*4).batch(BATCH_SIZE).map(lambda X: (en_fasttext_vectorizer(X[:,0]),hi_fasttext_vectorizer(X[:,1])))\n",
        "  tf_data = tf_data.map(lambda X_batch_en,X_batch_hi: ((X_batch_en,X_batch_hi[:,:-1]),X_batch_hi[:,1:]) )\n",
        "  tf_data = tf_data.prefetch(4)\n",
        "\n",
        "  return tf_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjIReY1bErnu",
        "outputId": "06503b8f-fec2-4d2b-8f01-d8d4335bd76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_dataset = create_tf_dataset(train)\n",
        "validation_dataset = create_tf_dataset(valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "96bvIypRbBQE"
      },
      "outputs": [],
      "source": [
        "del train,valid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "hJJvDg3cwUd9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ABYwlGoA5ySK"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = hi_fasttext_vectorizer.vocabulary_size()\n",
        "HIDDEN_UNITS = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "W4AB5UA1GIl5"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, encoder_units,embedding_layer,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder_units = encoder_units\n",
        "    self.embedding = embedding_layer\n",
        "    self.lstm = keras.layers.LSTM(self.encoder_units,return_sequences=True,return_state = True,recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self,input_tokens):\n",
        "    input_mask = tf.cast(input_tokens != 0, tf.float32)\n",
        "    input_vectors = self.embedding(input_tokens)\n",
        "    output,state_1,state_2 = self.lstm(input_vectors)\n",
        "    states = [state_1,state_2]\n",
        "    return output,states,input_mask"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    \n",
        "    self.Attention_Matrix = keras.layers.TimeDistributed(keras.layers.Dense(units,kernel_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self,encoded_input,decoded_output,input_mask=None):\n",
        "\n",
        "    encoded_input = self.Attention_Matrix(encoded_input)\n",
        "\n",
        "    if(input_mask!=None):\n",
        "      input_mask = tf.expand_dims(input_mask, axis=-1)\n",
        "      encoded_input = encoded_input*input_mask\n",
        "    \n",
        "    scores = tf.matmul(decoded_output,encoded_input,transpose_b = True)\n",
        "    distribution = tf.nn.softmax(scores)\n",
        "    output = tf.matmul(distribution, encoded_input)\n",
        "    \n",
        "    return output\n",
        "  "
      ],
      "metadata": {
        "id": "core9NYVQ6Wj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ocdzcz2KK8gG"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, decoder_units, embedding_layer, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.decoder_units = decoder_units\n",
        "    \n",
        "    self.embedding = embedding_layer\n",
        "    self.lstm = keras.layers.LSTM(self.decoder_units,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "    self.attention = Attention(decoder_units)\n",
        "\n",
        "    self.fc = keras.layers.TimeDistributed(keras.layers.Dense(self.output_vocab_size,kernel_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self,input_tokens,encoded_input, state = None,input_mask = None):\n",
        "    input_vectors = self.embedding(input_tokens)\n",
        "    output_ = self.lstm(input_vectors,initial_state=state)\n",
        "    attention_output_ = self.attention(encoded_input,output_,input_mask)\n",
        "    output = self.fc(attention_output_)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6dZoh6RGlpGZ"
      },
      "outputs": [],
      "source": [
        "class NMT(keras.Model):\n",
        "  def __init__(self, output_vocab_size,encoder_units,decoder_units, encoder_embedding_layer, decoder_embedding_layer, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = Encoder(encoder_units, encoder_embedding_layer)\n",
        "    self.decoder = Decoder(output_vocab_size, decoder_units, decoder_embedding_layer)\n",
        "\n",
        "  def call(self,input_tokens):\n",
        "    input_en_tokens,input_hi_tokens = input_tokens\n",
        "    encoder_output, encoder_final_state,input_mask = self.encoder(input_en_tokens)\n",
        "    output = self.decoder(input_hi_tokens, encoder_output,encoder_final_state,input_mask)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "V0FFOjI2pcKL"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred,sample_weight):\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(tf.reduce_sum(loss,axis = 1))\n",
        "\n",
        "masked_loss = MaskedLoss()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NMT_model = NMT(VOCAB_SIZE,HIDDEN_UNITS,HIDDEN_UNITS,en_embedding_layer,hi_embedding_layer)"
      ],
      "metadata": {
        "id": "1DOUCaRqw7pJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "WG12TM4VxGbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(1e-3)\n",
        "NMT_model.compile(optimizer = optimizer,loss = masked_loss)"
      ],
      "metadata": {
        "id": "veHiTX3YxC-H"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Early_Stopping = keras.callbacks.EarlyStopping(patience = 1,min_delta = 1,restore_best_weights=True)\n",
        "Model_Checkpoint = keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/LSTM_Attention.h5',save_best_only=True,save_weights_only=True)"
      ],
      "metadata": {
        "id": "0Z-ZW_1LxgdL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history_NMT = NMT_model.fit(training_dataset,epochs = 10,validation_data=validation_dataset,callbacks =[Early_Stopping,Model_Checkpoint,keras.callbacks.TerminateOnNaN()])\n"
      ],
      "metadata": {
        "id": "zDQAUF0nxnRC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Encoder and Decoder for inference"
      ],
      "metadata": {
        "id": "MQhOG0LUxtbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in training_dataset.take(1):\n",
        "  pass"
      ],
      "metadata": {
        "id": "5fZn6tX3DnCk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = NMT_model(item[0])\n",
        "NMT_model.load_weights('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/LSTM_Attention.h5')"
      ],
      "metadata": {
        "id": "SDw6eEDBDcXL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lrI_jQF2BAEC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qJU3-h64ooak"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(HIDDEN_UNITS,en_embedding_layer)\n",
        "_ = encoder(item[0][0])\n",
        "\n",
        "decoder = Decoder(VOCAB_SIZE,HIDDEN_UNITS,hi_embedding_layer)\n",
        "_ = decoder(item[0][1],_[0],_[1],_[2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.set_weights(NMT_model.get_weights()[:4])\n",
        "decoder.set_weights(NMT_model.get_weights()[4:])"
      ],
      "metadata": {
        "id": "CfIvgwfrNLC-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderModel(keras.Model):\n",
        "  def __init__(self,encoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "  \n",
        "  def call(self,input_tokens):\n",
        "    return self.encoder(input_tokens)\n",
        "\n",
        "\n",
        "class DecoderModel(keras.Model):\n",
        "  def __init__(self,decoder):\n",
        "    super().__init__()\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def call(self,input_tokens,encoded_input, state = None,input_mask = None):\n",
        "    return self.decoder(input_tokens,encoded_input, state = state,input_mask = input_mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "KpJvE0O7NLUO"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder_model = EncoderModel(encoder)\n",
        "Decoder_model = DecoderModel(decoder)"
      ],
      "metadata": {
        "id": "sRoR2dCUP89t"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = Encoder_model(item[0][0])\n",
        "_ = Decoder_model(item[0][1],_[0],_[1],_[2])"
      ],
      "metadata": {
        "id": "XnyzGY8zP-bW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder_model.save('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/Encoder_LSTM_Attention_tf', save_format = 'tf',)\n",
        "Decoder_model.save('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/Decoder_LSTM_Attention_tf', save_format = 'tf',)\n"
      ],
      "metadata": {
        "id": "uj-NPogZQAD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RgkjG-DARndA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}