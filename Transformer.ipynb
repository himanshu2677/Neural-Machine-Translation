{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp9aftXGBsF-"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "HIDDEN_UNITS = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mH7Hx3hvLi-",
        "outputId": "4d98dfd1-6e9e-41eb-8756-e2a4440c5f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vUwlEE0vJ5y"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5bKv1Ld0AGE"
      },
      "outputs": [],
      "source": [
        "processed_data_location = \"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/processed_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessed data, vectorizers and embedding layers for english and hindi text"
      ],
      "metadata": {
        "id": "N71DaW9uIAS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN4Oo_GuvShs"
      },
      "outputs": [],
      "source": [
        "with open(processed_data_location + \"/train_32.pkl\",'rb') as f:\n",
        "  train = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caitnkkk0Km-"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "random.shuffle(train)\n",
        "train_size = len(train)\n",
        "train, valid = train[:int(train_size*0.9)],train[int(train_size*0.9):]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = len(train)"
      ],
      "metadata": {
        "id": "C4M_C547bCPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JqQhaBDi4sY"
      },
      "outputs": [],
      "source": [
        "def load_vectorizer(location):\n",
        "  import pickle\n",
        "  from_disk = pickle.load(open(location, \"rb\"))\n",
        "  vectorizer = layers.TextVectorization.from_config(from_disk['config'])\n",
        "  vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "  vectorizer.set_weights(from_disk['weights'])\n",
        "  return vectorizer\n",
        "\n",
        "en_fasttext_vectorizer = load_vectorizer('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/EN_Fasttext_Vectorizer.pkl')\n",
        "hi_fasttext_vectorizer = load_vectorizer('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/HI_Fasttext_Vectorizer.pkl')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding(location):\n",
        "  import pickle\n",
        "  from_disk = pickle.load(open(location, \"rb\"))\n",
        "  embedding_layer = layers.Embedding.from_config(from_disk['config'])\n",
        "\n",
        "  return embedding_layer\n",
        "\n",
        "en_embedding_layer = load_embedding(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/en_fasttext_embedding_layer.pkl\")\n",
        "hi_embedding_layer = load_embedding(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/hi_fasttext_embedding_layer.pkl\")\n"
      ],
      "metadata": {
        "id": "8ALD461WIW4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow dataset"
      ],
      "metadata": {
        "id": "5sUSHGLTIarJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIO3awPCDkQs"
      },
      "outputs": [],
      "source": [
        "def create_tf_dataset(data):\n",
        "  data = [(text['en'],text['hi']) for text in data]\n",
        "  tf_data = tf.data.Dataset.from_tensor_slices(data)\n",
        "  tf_data = tf_data.shuffle(BATCH_SIZE*4).batch(BATCH_SIZE).map(lambda X: (en_fasttext_vectorizer(X[:,0]),hi_fasttext_vectorizer(X[:,1])))\n",
        "  tf_data = tf_data.map(lambda X_batch_en,X_batch_hi: ((X_batch_en,X_batch_hi[:,:-1]),X_batch_hi[:,1:]) )\n",
        "  tf_data = tf_data.prefetch(4)\n",
        "\n",
        "  return tf_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjIReY1bErnu",
        "outputId": "c12966ed-ec96-42c2-9861-d8c3524fcc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_dataset = create_tf_dataset(train)\n",
        "validation_dataset = create_tf_dataset(valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96bvIypRbBQE"
      },
      "outputs": [],
      "source": [
        "del train,valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABYwlGoA5ySK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "CQ7_3trZIrRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_TOKENS = 32\n",
        "N_LAYERS = 3 \n",
        "D_MODEL = 300\n",
        "DFF_UNITS = 512\n",
        "VOCAB_SIZE = hi_fasttext_vectorizer.vocabulary_size()\n",
        "DROPOUT = 0.2\n",
        "N_HEADS = 10\n"
      ],
      "metadata": {
        "id": "quhrzC-5I5am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4AB5UA1GIl5"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(keras.layers.Layer):\n",
        "    def __init__(self,max_steps,max_dims,dtype=tf.float32,**kwargs):\n",
        "      super().__init__(dtype=dtype,**kwargs)\n",
        "      self.max_steps = max_steps\n",
        "      self.max_dims = max_dims\n",
        "      # self.dtype = dtype\n",
        "      assert self.max_dims % 2 == 0\n",
        "\n",
        "      p,i = np.meshgrid(np.arange(self.max_steps),np.arange(self.max_dims // 2))\n",
        "\n",
        "      pos_embedding = np.zeros((1,self.max_steps,self.max_dims))\n",
        "      pos_embedding[0,:,::2] = np.sin(p / 10000**(2 * i/ self.max_dims)).T\n",
        "      pos_embedding[0,:, 1::2] = np.cos(p / 10000**(2 * i/ self.max_dims)).T\n",
        "      \n",
        "      self.pos_embedding = tf.constant(pos_embedding.astype(self.dtype))\n",
        "\n",
        "    def call(self,inputs):\n",
        "      shape = tf.shape(inputs)\n",
        "      return inputs + self.pos_embedding[:,:shape[-2],:shape[-1]]\n",
        "\n",
        "def Dot_Product_Attention(Q,K,V=None,Mask = None):\n",
        "\n",
        "  if V is None:\n",
        "    V = K\n",
        "  \n",
        "  qk = tf.matmul(Q,K,transpose_b = True)\n",
        "  dim_k = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "\n",
        "  scaled_qk = qk / tf.math.sqrt(dim_k)\n",
        "\n",
        "  if Mask is not None:\n",
        "    scaled_qk = scaled_qk + Mask * -1e9\n",
        "  \n",
        "  attention_weights = tf.nn.softmax(scaled_qk,axis = -1)\n",
        "\n",
        "  output = tf.matmul(attention_weights,V)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "  def __init__(self, d_model, n_heads,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    assert d_model % n_heads == 0\n",
        "\n",
        "    self.n_heads = n_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.depth = self.d_model // self.n_heads\n",
        "\n",
        "    self.wq = keras.layers.Dense(self.d_model,kernel_initializer='glorot_uniform')\n",
        "    self.wk = keras.layers.Dense(self.d_model,kernel_initializer='glorot_uniform')\n",
        "    self.wv = keras.layers.Dense(self.d_model,kernel_initializer='glorot_uniform')\n",
        "    \n",
        "    self.dense = keras.layers.Dense(self.d_model,kernel_initializer='glorot_uniform')\n",
        "    \n",
        "\n",
        "  def split_heads(self, x):\n",
        "    # print(x.shape)\n",
        "    # assert tf.rank(x) == 3\n",
        "    batch_size = tf.cast(tf.shape(x)[0],tf.int32)\n",
        "    x = tf.reshape(x, (batch_size,-1,self.n_heads,self.depth))\n",
        "    x = tf.transpose(x,[0,2,1,3])\n",
        "\n",
        "    return x\n",
        "\n",
        "  def call(self,Q,K,V = None,Mask = None):\n",
        "    batch_size = tf.shape(Q)[0]\n",
        "    \n",
        "    if V is None:\n",
        "      V = K\n",
        "\n",
        "    Q = self.wq(Q)\n",
        "    K = self.wk(K)\n",
        "    V = self.wv(V)\n",
        "\n",
        "    Q = self.split_heads(Q)\n",
        "    K = self.split_heads(K)\n",
        "    V = self.split_heads(V)\n",
        "\n",
        "    attention, attention_weights = Dot_Product_Attention(Q,K,V,Mask)\n",
        "    attention = tf.transpose(attention,[0,2,1,3])\n",
        "    attention = tf.reshape(attention,(batch_size,-1,self.d_model))\n",
        "\n",
        "    output = self.dense(attention)\n",
        "\n",
        "    return output, attention_weights\n",
        "    \n",
        "class FeedForwardsNetwork(keras.layers.Layer):\n",
        "  def __init__(self,d_model,dff,dropout,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "    self.dropout = dropout\n",
        "    self.layer1 = keras.layers.Dense(dff,activation = 'relu',kernel_initializer='glorot_uniform')\n",
        "    self.layer2 = keras.layers.Dense(d_model,kernel_initializer='glorot_uniform')\n",
        "    self.layer3 = keras.layers.Dropout(dropout)\n",
        "    \n",
        "  def call(self,x):\n",
        "    y1 = self.layer1(x)\n",
        "    y2 = self.layer2(y1)\n",
        "    output = self.layer3(y2)\n",
        "\n",
        "    return output\n",
        "\n",
        "class EncoderLayer(keras.layers.Layer):\n",
        "  def __init__(self, d_model,n_heads,dff,dropout,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    \n",
        "    self.mha = MultiHeadAttention(d_model = d_model, n_heads = n_heads)\n",
        "    self.ffn = FeedForwardsNetwork(d_model = d_model, dff = dff,dropout = dropout)\n",
        "\n",
        "    self.LayerNorm1 = keras.layers.Normalization()\n",
        "    self.LayerNorm2 = keras.layers.Normalization()\n",
        "\n",
        "  def call(self,x, Mask):\n",
        "    attention_output, _ = self.mha(x,x,x, Mask)\n",
        "    out1 = self.LayerNorm1(x + attention_output)\n",
        "\n",
        "    ffn_output = self.ffn(out1)\n",
        "    out2 = self.LayerNorm2(out1 + ffn_output)\n",
        "\n",
        "    return out2\n",
        "\n",
        "class DecoderLayer(keras.layers.Layer):\n",
        "  def __init__(self, d_model,n_heads,dff,dropout,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.dff = dff\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model = self.d_model, n_heads = self.n_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model = self.d_model, n_heads = self.n_heads)\n",
        "    self.ffn = FeedForwardsNetwork(d_model = self.d_model, dff = self.dff,dropout = self.dropout)\n",
        "\n",
        "    self.LayerNorm1 = keras.layers.Normalization()\n",
        "    self.LayerNorm2 = keras.layers.Normalization()\n",
        "    self.LayerNorm3 = keras.layers.Normalization()\n",
        "\n",
        "  def call(self,x,encoder_output,look_ahead_mask,padding_mask):\n",
        "\n",
        "    attention_out_1,_ = self.mha1(x,x,x,look_ahead_mask)\n",
        "    out1 = self.LayerNorm1(x + attention_out_1)\n",
        "\n",
        "    attention_out_2,_ = self.mha2(out1,encoder_output,encoder_output,padding_mask)\n",
        "    out2 = self.LayerNorm2(out1 + attention_out_2)\n",
        "\n",
        "    feedforward_out = self.ffn(out2)\n",
        "    out3 = self.LayerNorm3(out2 + feedforward_out)\n",
        "\n",
        "    return out3,attention_out_1,attention_out_2\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self,d_model,n_layers,n_heads,dff,dropout,vocab_size,en_embedding_layer,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.emb = en_embedding_layer\n",
        "    self.pos_emb = PositionalEncoding(MAX_TOKENS,d_model)\n",
        "\n",
        "    self.encoder_layers = [EncoderLayer(d_model = d_model,n_heads = n_heads,dff = dff,dropout = dropout) for i in range(self.n_layers)]\n",
        "\n",
        "  def call(self,x,mask):\n",
        "    embeddings = self.emb(x)\n",
        "    embeddings = self.pos_emb(embeddings)\n",
        "\n",
        "    output = embeddings\n",
        "    for encoder_layer in self.encoder_layers:\n",
        "      output = encoder_layer(output,mask)\n",
        "    \n",
        "    return output\n",
        "\n",
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self,d_model,n_layers,n_heads,dff,dropout,vocab_size,hi_embedding_layer,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    self.emb = hi_embedding_layer\n",
        "    self.pos_emb = PositionalEncoding(MAX_TOKENS,d_model)\n",
        "\n",
        "    self.decoder_layers = [DecoderLayer(d_model,n_heads,dff,dropout) for i in range(self.n_layers)]\n",
        "\n",
        "  def call(self,x,encoder_output,look_ahead_mask,padding_mask):\n",
        "    embeddings = self.emb(x)\n",
        "    embeddings = self.pos_emb(embeddings)\n",
        "    attention_weights = {}\n",
        "    output = embeddings\n",
        "    for i,decoder_layer in enumerate(self.decoder_layers):\n",
        "      output,block1,block2 = decoder_layer(output,encoder_output,look_ahead_mask,padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "\n",
        "    return output,attention_weights\n",
        "\n",
        "@tf.function\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "@tf.function\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cs9w2__BCfD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(keras.models.Model):\n",
        "  def __init__(self,n_layers,d_model,n_heads,dff,vocab_size,dropout,en_embedding_layer,hi_embedding_layer,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    \n",
        "    self.n_layers = n_layers\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.dff = dff\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.encoder = Encoder(d_model = d_model,n_layers = n_layers,n_heads = n_heads,dff = dff,dropout = dropout,vocab_size = vocab_size,en_embedding_layer = hi_embedding_layer)\n",
        "    self.decoder = Decoder(d_model = d_model,n_layers = n_layers,n_heads = n_heads,dff = dff,dropout = dropout,vocab_size = vocab_size,hi_embedding_layer = en_embedding_layer)\n",
        "    \n",
        "    self.final_layer = tf.keras.layers.Dense(vocab_size,kernel_initializer='glorot_uniform')\n",
        "\n",
        "  def create_masks(self,inp,tar):\n",
        "\n",
        "    padding_mask = create_padding_mask(inp)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    padding_mask_target = create_padding_mask(tar)\n",
        "\n",
        "    look_ahead_mask = tf.maximum(look_ahead_mask,padding_mask_target)\n",
        "\n",
        "    return padding_mask,look_ahead_mask\n",
        "  \n",
        "  def call(self,input):\n",
        "    inp,tar = input\n",
        "    padding_mask,look_ahead_mask = self.create_masks(inp,tar)\n",
        "\n",
        "    encoder_output = self.encoder(inp,padding_mask)\n",
        "\n",
        "    decoder_output,attention_weights = self.decoder(tar,encoder_output,look_ahead_mask,padding_mask)\n",
        "\n",
        "    final_output = self.final_layer(decoder_output)\n",
        "\n",
        "    return final_output\n",
        "    "
      ],
      "metadata": {
        "id": "m4kL__2CCe_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = N_LAYERS\n",
        "d_model = D_MODEL\n",
        "dff = DFF_UNITS\n",
        "vocab_size = VOCAB_SIZE\n",
        "dropout = DROPOUT\n",
        "n_heads = N_HEADS\n",
        "\n",
        "transformer = Transformer(n_layers=n_layers, d_model=d_model, n_heads=n_heads, dff=dff,vocab_size=vocab_size,dropout = dropout,en_embedding_layer = en_embedding_layer,hi_embedding_layer = hi_embedding_layer)\n"
      ],
      "metadata": {
        "id": "sPWNerOfCe5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0FFOjI2pcKL"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred,sample_weight):\n",
        "    # y_pred = y_pred[0]\n",
        "    # print(\"pred shape: \",tf.shape(y_pred))\n",
        "    # print(\"true shape: \",tf.shape(y_true))\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(tf.reduce_sum(loss,axis = 1))\n",
        "\n",
        "masked_loss = MaskedLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "CGYEwop6J5hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Early_Stopping = keras.callbacks.EarlyStopping(patience = 3,min_delta = 1,restore_best_weights=True)\n",
        "Model_Checkpoint = keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/Transformer.h5',save_best_only=True,save_weights_only=True)\n",
        "\n",
        "optimizer = keras.optimizers.Adam(5e-5)\n",
        "transformer.compile(optimizer = optimizer,loss = masked_loss)\n"
      ],
      "metadata": {
        "id": "6A6AalFKPM8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXOB7-Iva7Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ghkEt4vtAO6J"
      },
      "outputs": [],
      "source": [
        "history_NMT = transformer.fit(training_dataset.repeat(),steps_per_epoch=train_size//(BATCH_SIZE*5),epochs = 15,validation_data=validation_dataset,callbacks =[Early_Stopping,Model_Checkpoint,keras.callbacks.TerminateOnNaN()])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}