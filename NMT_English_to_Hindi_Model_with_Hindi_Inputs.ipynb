{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128"
      ],
      "metadata": {
        "id": "0OlT4yIc-rFf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mH7Hx3hvLi-",
        "outputId": "f3797cae-2c62-47d4-8aeb-c90acdfeff31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7vUwlEE0vJ5y"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N5bKv1Ld0AGE"
      },
      "outputs": [],
      "source": [
        "processed_data_location = \"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/processed_data\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessed data, vectorizers and embedding layers for english and hindi text\n"
      ],
      "metadata": {
        "id": "yXFZV61hrdy1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GN4Oo_GuvShs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bc3e26-85a1-4bfe-84aa-a7ce06fad86a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training examples: 1448877\n"
          ]
        }
      ],
      "source": [
        "with open(processed_data_location + \"/train.pkl\",'rb') as f:\n",
        "  train = pickle.load(f)\n",
        "\n",
        "print(\"Number of Training examples:\", len(train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "caitnkkk0Km-"
      },
      "outputs": [],
      "source": [
        "random.seed(10)\n",
        "random.shuffle(train)\n",
        "train_size = len(train)\n",
        "train, valid = train[:int(train_size*0.9)],train[int(train_size*0.9):]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4JqQhaBDi4sY"
      },
      "outputs": [],
      "source": [
        "def load_vectorizer(location):\n",
        "  import pickle\n",
        "  from_disk = pickle.load(open(location, \"rb\"))\n",
        "  vectorizer = layers.TextVectorization.from_config(from_disk['config'])\n",
        "  vectorizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "  vectorizer.set_weights(from_disk['weights'])\n",
        "  return vectorizer\n",
        "\n",
        "en_fasttext_vectorizer = load_vectorizer('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/EN_Fasttext_Vectorizer.pkl')\n",
        "hi_fasttext_vectorizer = load_vectorizer('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/HI_Fasttext_Vectorizer.pkl')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding(location):\n",
        "  import pickle\n",
        "  from_disk = pickle.load(open(location, \"rb\"))\n",
        "  embedding_layer = layers.Embedding.from_config(from_disk['config'])\n",
        "\n",
        "  return embedding_layer\n",
        "\n",
        "en_embedding_layer = load_embedding(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/en_fasttext_embedding_layer.pkl\")\n",
        "hi_embedding_layer = load_embedding(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/hi_fasttext_embedding_layer.pkl\")\n"
      ],
      "metadata": {
        "id": "bsORyn_wrxpw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensorflow dataset"
      ],
      "metadata": {
        "id": "i1l8LH8fr67A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UIO3awPCDkQs"
      },
      "outputs": [],
      "source": [
        "def create_tf_dataset(data):\n",
        "  data = [(text['en'],text['hi']) for text in data]\n",
        "  tf_data = tf.data.Dataset.from_tensor_slices(data)\n",
        "  tf_data = tf_data.shuffle(BATCH_SIZE*4).batch(BATCH_SIZE).map(lambda X: (en_fasttext_vectorizer(X[:,0]),hi_fasttext_vectorizer(X[:,1])))\n",
        "  tf_data = tf_data.map(lambda X_batch_en,X_batch_hi: ((X_batch_en,X_batch_hi[:,:-1]),X_batch_hi[:,1:]) )\n",
        "  tf_data = tf_data.prefetch(4)\n",
        "\n",
        "  return tf_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjIReY1bErnu",
        "outputId": "3c262269-29f6-44ea-b57d-30a83f34826a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_dataset = create_tf_dataset(train)\n",
        "validation_dataset = create_tf_dataset(valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "96bvIypRbBQE"
      },
      "outputs": [],
      "source": [
        "del train,valid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "JcpLzgQmt1Zx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h9Qs71BLFTMH"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = hi_fasttext_vectorizer.vocabulary_size()\n",
        "HIDDEN_UNITS = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W4AB5UA1GIl5"
      },
      "outputs": [],
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, encoder_units,embedding_layer,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder_units = encoder_units\n",
        "    self.embedding = embedding_layer\n",
        "    self.lstm = keras.layers.LSTM(self.encoder_units,return_state = True,recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self,input_tokens):\n",
        "    input_vectors = self.embedding(input_tokens)\n",
        "    output,state_1,state_2 = self.lstm(input_vectors)\n",
        "    states = [state_1,state_2]\n",
        "    return output,states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ocdzcz2KK8gG"
      },
      "outputs": [],
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, decoder_units, embedding_layer, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.decoder_units = decoder_units\n",
        "    \n",
        "    self.embedding = embedding_layer\n",
        "    self.lstm = keras.layers.LSTM(self.decoder_units,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.fc = keras.layers.TimeDistributed(keras.layers.Dense(self.output_vocab_size,kernel_initializer='glorot_uniform'))\n",
        "\n",
        "  def call(self,input_tokens,input_state):\n",
        "    input_vectors = self.embedding(input_tokens)\n",
        "    output_ = self.lstm(input_vectors,initial_state=input_state)\n",
        "    output = self.fc(output_)\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6dZoh6RGlpGZ"
      },
      "outputs": [],
      "source": [
        "class NMT(keras.Model):\n",
        "  def __init__(self, output_vocab_size,encoder_units,decoder_units, encoder_embedding_layer, decoder_embedding_layer, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.encoder = Encoder(encoder_units, encoder_embedding_layer)\n",
        "    self.decoder = Decoder(output_vocab_size, decoder_units, decoder_embedding_layer)\n",
        "\n",
        "  def call(self,input_tokens):\n",
        "    input_en_tokens,input_hi_tokens = input_tokens\n",
        "    encoder_output, encoder_final_state = self.encoder(input_en_tokens)\n",
        "    output = self.decoder(input_hi_tokens, encoder_final_state)\n",
        "    # print(\"output shape: \", tf.shape(output))\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred,sample_weight):\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_mean(tf.reduce_sum(loss,axis = 1))\n",
        "\n",
        "masked_loss = MaskedLoss()"
      ],
      "metadata": {
        "id": "TmhHofwUe0hP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NMT_model = NMT(VOCAB_SIZE,HIDDEN_UNITS,HIDDEN_UNITS,en_embedding_layer,hi_embedding_layer)"
      ],
      "metadata": {
        "id": "q2DsO8DSuKfR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "RdEdf8dHufRq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "V0FFOjI2pcKL"
      },
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "NMT_model.compile(optimizer = optimizer,loss = masked_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Early_Stopping = keras.callbacks.EarlyStopping(patience = 2,min_delta = 0.5,restore_best_weights=True)\n",
        "Model_Checkpoint = keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/LSTM.h5',save_best_only=True,save_weights_only=True)\n"
      ],
      "metadata": {
        "id": "5fOlIOijOSkb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ghkEt4vtAO6J"
      },
      "outputs": [],
      "source": [
        "# history_NMT = NMT_model.fit(training_dataset,epochs = 10,validation_data=validation_dataset,callbacks =[Early_Stopping,Model_Checkpoint,keras.callbacks.TerminateOnNaN()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Encoder and Decoder for inference\n",
        "\n"
      ],
      "metadata": {
        "id": "BDnIXcNBuqv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for item in training_dataset.take(1):\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "CgHeAJd3h7KT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = NMT_model(item[0])\n",
        "NMT_model.load_weights('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/LSTM.h5')"
      ],
      "metadata": {
        "id": "5evhCniYEP5g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(HIDDEN_UNITS,en_embedding_layer)\n",
        "_ = encoder(item[0][0])\n",
        "\n",
        "decoder = Decoder(VOCAB_SIZE,HIDDEN_UNITS,hi_embedding_layer)\n",
        "_ = decoder(item[0][1],_[1])"
      ],
      "metadata": {
        "id": "pbXhj6R5g1k4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.set_weights(NMT_model.get_weights()[:4])\n",
        "decoder.set_weights(NMT_model.get_weights()[4:])"
      ],
      "metadata": {
        "id": "R8UxdR5ZhPFD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderModel(keras.Model):\n",
        "  def __init__(self,encoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "  \n",
        "  def call(self,input_tokens):\n",
        "    return self.encoder(input_tokens)\n",
        "\n",
        "\n",
        "class DecoderModel(keras.Model):\n",
        "  def __init__(self,decoder):\n",
        "    super().__init__()\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def call(self,input_tokens,input_state):\n",
        "    return self.decoder(input_tokens,input_state)\n",
        "\n"
      ],
      "metadata": {
        "id": "k0zdj143o1I6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder_model = EncoderModel(encoder)\n",
        "Decoder_model = DecoderModel(decoder)"
      ],
      "metadata": {
        "id": "s66GhfOfohqu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = Encoder_model(item[0][0])\n",
        "_ = Decoder_model(item[0][1],_[1])"
      ],
      "metadata": {
        "id": "3i88eaZ0pxOi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Encoder_model.save('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/Encoder_LSTM_tf', save_format = 'tf',)\n",
        "Decoder_model.save('/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/NMT_Models/Decoder_LSTM_tf', save_format = 'tf',)"
      ],
      "metadata": {
        "id": "_WxF3fJZnh-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}