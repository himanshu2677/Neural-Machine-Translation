{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdbqvu_doiM_",
        "outputId": "0a4927db-28e4-4d88-9026-c6281c8be35d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing huggingface library 'dataset'. English to Hindi Translation data can be downloaded through this library.\n"
      ],
      "metadata": {
        "id": "z0INVTjidvO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiXpVp2foOSz",
        "outputId": "ac329af7-ee9f-4bae-e518-f845b8b801ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
            "\u001b[K     |████████████████████████████████| 452 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 56.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.8.0 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4b71YdD5zTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "IXGy22s4oYW2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_location = \"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/Raw_data\"\n",
        "processed_data_location = \"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/processed_data\""
      ],
      "metadata": {
        "id": "Vy-Tnhl_tYS_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9g08SxJaeYmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch Data"
      ],
      "metadata": {
        "id": "0IeyL3ocmHgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and save dataset released by IIT Bombay for english-hindi translation."
      ],
      "metadata": {
        "id": "y278xvZ5ebvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"cfilt/iitb-english-hindi\")\n",
        "dataset.save_to_disk(raw_data_location)"
      ],
      "metadata": {
        "id": "uNRPBOc3ozL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_from_disk(raw_data_location)"
      ],
      "metadata": {
        "id": "Hi78v6FHsk5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0532e06c-96c7-4217-a049-6186aab5e559"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']['translation'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oGvR5A1ptMA",
        "outputId": "06fa4af8-f5c2-4011-fb22-f1bc8a31c284"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'en': 'Give your application an accessibility workout',\n",
              "  'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'},\n",
              " {'en': 'Accerciser Accessibility Explorer',\n",
              "  'hi': 'एक्सेर्साइसर पहुंचनीयता अन्वेषक'},\n",
              " {'en': 'The default plugin layout for the bottom panel',\n",
              "  'hi': 'निचले पटल के लिए डिफोल्ट प्लग-इन खाका'},\n",
              " {'en': 'The default plugin layout for the top panel',\n",
              "  'hi': 'ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका'},\n",
              " {'en': 'A list of plugins that are disabled by default',\n",
              "  'hi': 'उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से निष्क्रिय किया गया है'}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fuXuSQ5hesD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing data"
      ],
      "metadata": {
        "id": "IoUWAKkoes0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_english_text(text):\n",
        "  text = str(text).lower()\n",
        "  text = text.replace('\\n','')\n",
        "  text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "  text = re.sub(r'\\d+','NUM',text)\n",
        "  text = re.sub(r'\\s+',' ',text)\n",
        "  text = text.strip()\n",
        "  return text\n",
        "\n",
        "def process_hindi_text(text):\n",
        "  text = str(text).lower()\n",
        "  text = text.replace('\\n','')\n",
        "  text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "  text = re.sub(r'\\d+','NUM',text)\n",
        "  text = re.sub(r'\\s+',' ',text)\n",
        "  text = text.strip()\n",
        "  text = re.sub(r'[a-zA-Z]','',text)\n",
        "  return text\n",
        "\n",
        "def process_text(en_hi_text_dict):\n",
        "  en_text = '<START> ' + process_english_text(en_hi_text_dict['en']) + ' <END>'\n",
        "  hi_text = '<START> ' + process_hindi_text(en_hi_text_dict['hi']) + ' <END>'\n",
        "\n",
        "  return {'en': en_text,'hi': hi_text}\n",
        "\n",
        "def process_texts(data):\n",
        "  processed_data = []\n",
        "  for en_hi_text_dict in data:\n",
        "    processed_data.append(process_text(en_hi_text_dict))\n",
        "\n",
        "  return processed_data"
      ],
      "metadata": {
        "id": "KSXB4lscqzAn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = process_texts(dataset['train']['translation'])\n",
        "valid = process_texts(dataset['validation']['translation'])\n",
        "test = process_texts(dataset['test']['translation'])"
      ],
      "metadata": {
        "id": "Ob3MVC3sx318"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del dataset"
      ],
      "metadata": {
        "id": "SpiKzLZ-LKLx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to first decide what length of texts are we going to work on."
      ],
      "metadata": {
        "id": "h39JFDXIe-F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lengths(en_hi_texts):\n",
        "  en_length = []\n",
        "  hi_length = []\n",
        "\n",
        "  for en_hi_text in en_hi_texts:\n",
        "    en = en_hi_text['en']\n",
        "    hi = en_hi_text['hi']\n",
        "\n",
        "    en_length.append(len(en.split()))\n",
        "    hi_length.append(len(hi.split()))\n",
        "  \n",
        "  return en_length,hi_length\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "xVCXqNMOyIjW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_length,hi_length = get_lengths(train)"
      ],
      "metadata": {
        "id": "902zPnHdzLBh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(sum(np.array(en_length) <= 32) / len(en_length)*100,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyuknJp1zoqc",
        "outputId": "21af63de-9866-4478-f872-48875783c800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.47"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round(sum(np.array(hi_length) <= 32) / len(hi_length)*100,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlOFE34UzrjC",
        "outputId": "f2523111-2681-4223-9893-589c54ddc6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.59"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round(sum((np.array(en_length) <= 32)*(np.array(hi_length) <= 32)) / len(en_length)*100,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8A-9UJrzuwk",
        "outputId": "0083e034-14ba-4d66-97e1-07de735bce29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87.33"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "87.3% of the english-hindi pairs have length less than 32 for both english and hindi sentences. Hence, length of 32 is apt for our translation model."
      ],
      "metadata": {
        "id": "bYdTwNW_fKMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_on_length(en_hi_texts):\n",
        "  filtered_en_hi_texts = []\n",
        "  for en_hi_text in en_hi_texts:\n",
        "    if(len(en_hi_text['en'].split()) <= 32):\n",
        "      if(len(en_hi_text['hi'].split()) <= 32):\n",
        "        filtered_en_hi_texts.append(en_hi_text)\n",
        "  \n",
        "  return filtered_en_hi_texts"
      ],
      "metadata": {
        "id": "heukbzgM7Zwk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = filter_on_length(train)\n",
        "valid = filter_on_length(valid)\n",
        "test = filter_on_length(test)\n"
      ],
      "metadata": {
        "id": "mxv1fxyR9AJY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving final english-hindi pairs with length less than or equal to 32 for both english and hindi sentences."
      ],
      "metadata": {
        "id": "afrVjDMvfkom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(processed_data_location + \"/train.pkl\",'wb') as f:\n",
        "  pickle.dump(train,f)\n",
        "with open(processed_data_location + \"/valid.pkl\",'wb') as f:\n",
        "  pickle.dump(valid,f)\n",
        "with open(processed_data_location + \"/test.pkl\",'wb') as f:\n",
        "  pickle.dump(test,f)\n"
      ],
      "metadata": {
        "id": "JSNxzJhH7BJE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train),len(valid),len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdrqkOFU9JEN",
        "outputId": "54ce2ac6-ce35-4f2f-fd41-602eb34254b0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1448877, 465, 1942)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train,valid,test"
      ],
      "metadata": {
        "id": "dXEbsJ9yfy9H"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizer and Embedding"
      ],
      "metadata": {
        "id": "8CYMBBbah8Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 2e5\n",
        "OOV_TOKEN = 1\n",
        "START_TOKEN = 2\n",
        "END_TOKEN = 3\n",
        "MAX_LENGTH = 32"
      ],
      "metadata": {
        "id": "8Qzu6wIPm0iq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Fasttext vectors for english and hindi words."
      ],
      "metadata": {
        "id": "8uT8_vaFgua0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip -P \"/content/drive/MyDrive/MachineLearning/fasttext_en_cc\"\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz -P \"/content/drive/MyDrive/MachineLearning/fasttext_hi\""
      ],
      "metadata": {
        "id": "kJEdfGR4GK52"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/MachineLearning/fasttext_en_cc/crawl-300d-2M.vec.zip\"\n",
        "!gunzip -q \"/content/drive/MyDrive/MachineLearning/fasttext_hi/cc.hi.300.vec.gz\""
      ],
      "metadata": {
        "id": "dveqOOqbbyay"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from zipfile import ZipFile\n",
        "import numpy as np\n",
        "\n",
        "def load_vectors(fname,VOCAB_SIZE):\n",
        "  fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "  n, d = map(int, fin.readline().split())\n",
        "  data = {}\n",
        "  i = 0\n",
        "  for line in fin:\n",
        "      tokens = line.rstrip().split(' ')\n",
        "      word,vector = tokens[0].lower(), [float(t) for t in tokens[1:]]\n",
        "      if word not in data:\n",
        "        data[word] = [vector]\n",
        "        i = i + 1\n",
        "      else:\n",
        "        data[word].append(vector)\n",
        "      if (i == VOCAB_SIZE):\n",
        "        break\n",
        "  for word in data:\n",
        "    data[word] = np.mean(data[word],axis = 0)\n",
        "  return data"
      ],
      "metadata": {
        "id": "Flq6-oHab3pV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_vectors = load_vectors('/content/crawl-300d-2M.vec',VOCAB_SIZE)\n",
        "hi_vectors = load_vectors('/content/drive/MyDrive/MachineLearning/fasttext_hi/cc.hi.300.vec',VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "Ze5TNog8ggza"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_vocab = list(en_vectors.keys())\n",
        "hi_vocab = list(hi_vectors.keys())\n",
        "\n",
        "en_vocab = [\"\",\"[UNK]\",\"<START>\",\"<END>\"] + [word for word in en_vocab if word not in [\"\",\"[UNK]\",\"<START>\",\"<END>\"]][:-4]\n",
        "hi_vocab = [\"\",\"[UNK]\",\"<START>\",\"<END>\"] + [word for word in hi_vocab if word not in [\"\",\"[UNK]\",\"<START>\",\"<END>\"]][:-4]"
      ],
      "metadata": {
        "id": "qEWdfuZ9xXhB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing vectorizer layer"
      ],
      "metadata": {
        "id": "YOqeCO-5imnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "en_fasttext_vectorizer = layers.TextVectorization(standardize = None,\n",
        "    output_mode='int',output_sequence_length = MAX_LENGTH)\n",
        "\n",
        "hi_fasttext_vectorizer = layers.TextVectorization(standardize = None,\n",
        "    output_mode='int',output_sequence_length = MAX_LENGTH)\n"
      ],
      "metadata": {
        "id": "Tyol7852qkJm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_fasttext_vectorizer.set_vocabulary(np.array(en_vocab))\n",
        "hi_fasttext_vectorizer.set_vocabulary(np.array(hi_vocab))\n"
      ],
      "metadata": {
        "id": "drTO40qEqfqT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump({'config': en_fasttext_vectorizer.get_config(),\n",
        "             'weights': en_fasttext_vectorizer.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/EN_Fasttext_Vectorizer.pkl\", \"wb\"))\n",
        "\n",
        "import pickle\n",
        "pickle.dump({'config': hi_fasttext_vectorizer.get_config(),\n",
        "             'weights': hi_fasttext_vectorizer.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/HI_Fasttext_Vectorizer.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "KnSxYewXqjGp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing embedding layer for english and hindi text"
      ],
      "metadata": {
        "id": "ZRFOPlh4i3ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding_layer(vectorizer,embeddings):\n",
        "  voc = vectorizer.get_vocabulary()\n",
        "  word_index = dict(zip(voc, range(len(voc))))\n",
        "\n",
        "  OOV_vector = embeddings.get(voc[-1])\n",
        "  START_vector = embeddings.get(voc[-2])\n",
        "  END_vector = embeddings.get(voc[-3])\n",
        "\n",
        "  num_tokens = len(voc) + 2 \n",
        "  embedding_dim = 300\n",
        "  hits = 0\n",
        "  misses = 0\n",
        "  missed_words = []\n",
        "  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      embedding_vector = embeddings.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "          hits += 1\n",
        "      else:\n",
        "          missed_words.append(word)\n",
        "          misses += 1\n",
        "  print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "  embedding_matrix[OOV_TOKEN] = OOV_vector\n",
        "  embedding_matrix[START_TOKEN] = START_vector\n",
        "  embedding_matrix[END_TOKEN] = END_vector\n",
        "\n",
        "\n",
        "  embedding_layer = layers.Embedding(\n",
        "      num_tokens,\n",
        "      embedding_dim,\n",
        "      embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
        "      trainable=False,\n",
        "      name = 'embedding_layer_300'\n",
        "  )\n",
        "\n",
        "  return embedding_layer\n",
        "\n"
      ],
      "metadata": {
        "id": "_eYC7Cj0kn05"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_embedding_layer = get_embedding_layer(en_fasttext_vectorizer,en_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikVORNDNny18",
        "outputId": "c4046fa9-c682-406b-ad52-ff9aeef26315"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 199996 words (4 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_embedding_layer = get_embedding_layer(en_fasttext_vectorizer,en_vectors)\n",
        "\n",
        "import pickle\n",
        "pickle.dump({'config': en_embedding_layer.get_config(),\n",
        "             'weights': en_embedding_layer.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/en_fasttext_embedding_layer.pkl\", \"wb\"))\n",
        "   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ov0yQ9NqJS_",
        "outputId": "deffd2ae-60e8-480d-fca2-6f12fc5f1526"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 199996 words (4 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hi_embedding_layer = get_embedding_layer(hi_fasttext_vectorizer,hi_vectors)\n",
        "\n",
        "import pickle\n",
        "pickle.dump({'config': hi_embedding_layer.get_config(),\n",
        "             'weights': hi_embedding_layer.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/MachineLearning/NMT_English_to_Hindi/hi_fasttext_embedding_layer.pkl\", \"wb\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lWjS1qRsO8N",
        "outputId": "cdf216b2-9dfc-433a-b61e-f48cbe3b0028"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 199996 words (4 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gcWDt7YG6sgP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}